{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.1 in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (5.1)\n",
      "fatal: destination path 'detectron2' already exists and is not an empty directory.\n",
      "Ignoring dataclasses: markers 'python_version < \"3.7\"' don't match your environment\n",
      "Requirement already satisfied: Pillow>=7.1 in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (11.0.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pycocotools>=2.0.2\n",
      "  Downloading pycocotools-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting termcolor>=1.1\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting yacs>=0.1.8\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tqdm>4.29.0\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting fvcore<0.1.6,>=0.1.5\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting iopath<0.1.10,>=0.1.7\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Collecting omegaconf<2.4,>=2.1\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting hydra-core>=1.1\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting black\n",
      "  Downloading black-24.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Requirement already satisfied: packaging in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (24.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (from matplotlib) (2.2.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: PyYAML in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (from yacs>=0.1.8) (5.1)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (from tensorboard) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4,>=2.1)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting click>=8.0.0 (from black)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mypy-extensions>=0.4.3 (from black)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (from black) (4.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading pycocotools-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Downloading black-24.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=4651a9c58db4c2bcf533b5f2b373d8ed1366bcf64b09f51ae474335574476597\n",
      "  Stored in directory: /home/shuai/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=aeb6776759a5a3ae544676dfad159c6a703c3a89da9827b0bf5b6acb57394bf6\n",
      "  Stored in directory: /home/shuai/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built fvcore antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, werkzeug, tqdm, termcolor, tensorboard-data-server, tabulate, pyparsing, protobuf, portalocker, pathspec, omegaconf, mypy-extensions, markdown, kiwisolver, grpcio, fonttools, cycler, contourpy, cloudpickle, click, absl-py, tensorboard, matplotlib, iopath, hydra-core, black, pycocotools, fvcore\n",
      "Successfully installed absl-py-2.1.0 antlr4-python3-runtime-4.9.3 black-24.10.0 click-8.1.8 cloudpickle-3.1.0 contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 fvcore-0.1.5.post20221221 grpcio-1.68.1 hydra-core-1.3.2 iopath-0.1.9 kiwisolver-1.4.8 markdown-3.7 matplotlib-3.10.0 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-3.1.1 protobuf-5.29.2 pycocotools-2.0.8 pyparsing-3.2.1 tabulate-0.9.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 termcolor-2.5.0 tqdm-4.67.1 werkzeug-3.1.3 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pyyaml==5.1\n",
    "import sys, os, distutils.core\n",
    "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "# Properly install detectron2. (Please do not install twice in both ways)\n",
    "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvcc\n",
      "torch:  2.5 ; cuda:  cu124\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import sys, os, distutils.core\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shuai/blasting\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"my_dataset_train\", {\"mask_format\": \"bitmask\"}, \"/home/shuai/blasting/Dataset/train_dataset_coco/annotations.json\", \"/home/shuai/blasting/Dataset/train_dataset_coco\")\n",
    "register_coco_instances(\"my_dataset_val\", {\"mask_format\": \"bitmask\"}, \"/home/shuai/blasting/Dataset/val_dataset_coco/annotations.json\", \"/home/shuai/blasting/Dataset/val_dataset_coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4  # This is the real \"batch size\" commonly known to deep learning people\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 1200    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class. (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "# cfg.MODEL.DEVICE = \"cpu\"\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "\n",
    "cfg.INPUT.MASK_FORMAT = \"bitmask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/11 22:04:36 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[01/11 22:04:36 d2.data.datasets.coco]: \u001b[0mLoaded 52 images in COCO format from /home/shuai/blasting/Dataset/train_dataset_coco/annotations.json\n",
      "\u001b[32m[01/11 22:04:36 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 52 images left.\n",
      "\u001b[32m[01/11 22:04:36 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   smoke    | 472          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[01/11 22:04:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[01/11 22:04:36 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[01/11 22:04:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/11 22:04:36 d2.data.common]: \u001b[0mSerializing 52 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/11 22:04:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.12 MiB\n",
      "\u001b[32m[01/11 22:04:36 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=4\n",
      "\u001b[32m[01/11 22:04:36 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/11 22:04:36 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuai/blasting/detectron2/detectron2/data/detection_utils.py:449: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
      "/home/shuai/blasting/detectron2/detectron2/data/detection_utils.py:449: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
      "/home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/11 22:04:44 d2.utils.events]: \u001b[0m eta: 0:05:48  iter: 19  total_loss: 2.522  loss_cls: 0.741  loss_box_reg: 0.5715  loss_mask: 0.6949  loss_rpn_cls: 0.4031  loss_rpn_loc: 0.08533    time: 0.3299  last_time: 0.4037  data_time: 0.0638  last_data_time: 0.1443   lr: 4.9953e-06  max_mem: 5294M\n",
      "\u001b[32m[01/11 22:04:50 d2.utils.events]: \u001b[0m eta: 0:05:34  iter: 39  total_loss: 2.418  loss_cls: 0.6784  loss_box_reg: 0.6677  loss_mask: 0.6905  loss_rpn_cls: 0.2746  loss_rpn_loc: 0.0992    time: 0.3233  last_time: 0.2790  data_time: 0.0404  last_data_time: 0.0103   lr: 9.9902e-06  max_mem: 5294M\n",
      "\u001b[32m[01/11 22:04:57 d2.utils.events]: \u001b[0m eta: 0:05:35  iter: 59  total_loss: 2.176  loss_cls: 0.5939  loss_box_reg: 0.6778  loss_mask: 0.6806  loss_rpn_cls: 0.136  loss_rpn_loc: 0.07679    time: 0.3233  last_time: 0.3258  data_time: 0.0421  last_data_time: 0.0561   lr: 1.4985e-05  max_mem: 5294M\n",
      "\u001b[32m[01/11 22:05:04 d2.utils.events]: \u001b[0m eta: 0:05:30  iter: 79  total_loss: 2.154  loss_cls: 0.5441  loss_box_reg: 0.7652  loss_mask: 0.6672  loss_rpn_cls: 0.06445  loss_rpn_loc: 0.09399    time: 0.3272  last_time: 0.3323  data_time: 0.0583  last_data_time: 0.0429   lr: 1.998e-05  max_mem: 5294M\n",
      "\u001b[32m[01/11 22:05:10 d2.utils.events]: \u001b[0m eta: 0:05:25  iter: 99  total_loss: 2.041  loss_cls: 0.5037  loss_box_reg: 0.7169  loss_mask: 0.6505  loss_rpn_cls: 0.05911  loss_rpn_loc: 0.06898    time: 0.3262  last_time: 0.4231  data_time: 0.0409  last_data_time: 0.1293   lr: 2.4975e-05  max_mem: 5294M\n",
      "\u001b[32m[01/11 22:05:17 d2.utils.events]: \u001b[0m eta: 0:05:21  iter: 119  total_loss: 1.962  loss_cls: 0.4725  loss_box_reg: 0.6996  loss_mask: 0.6298  loss_rpn_cls: 0.06275  loss_rpn_loc: 0.08024    time: 0.3279  last_time: 0.2722  data_time: 0.0582  last_data_time: 0.0124   lr: 2.997e-05  max_mem: 5294M\n",
      "\u001b[32m[01/11 22:05:23 d2.utils.events]: \u001b[0m eta: 0:05:15  iter: 139  total_loss: 1.882  loss_cls: 0.4467  loss_box_reg: 0.6806  loss_mask: 0.6074  loss_rpn_cls: 0.04902  loss_rpn_loc: 0.08515    time: 0.3260  last_time: 0.2834  data_time: 0.0373  last_data_time: 0.0088   lr: 3.4965e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:05:30 d2.utils.events]: \u001b[0m eta: 0:05:08  iter: 159  total_loss: 1.882  loss_cls: 0.4477  loss_box_reg: 0.7207  loss_mask: 0.5803  loss_rpn_cls: 0.04169  loss_rpn_loc: 0.06947    time: 0.3250  last_time: 0.2956  data_time: 0.0382  last_data_time: 0.0118   lr: 3.996e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:05:36 d2.utils.events]: \u001b[0m eta: 0:05:03  iter: 179  total_loss: 1.797  loss_cls: 0.4179  loss_box_reg: 0.6635  loss_mask: 0.5542  loss_rpn_cls: 0.04406  loss_rpn_loc: 0.08974    time: 0.3251  last_time: 0.2924  data_time: 0.0467  last_data_time: 0.0106   lr: 4.4955e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:05:43 d2.utils.events]: \u001b[0m eta: 0:04:57  iter: 199  total_loss: 1.767  loss_cls: 0.4274  loss_box_reg: 0.7122  loss_mask: 0.5223  loss_rpn_cls: 0.04441  loss_rpn_loc: 0.06638    time: 0.3255  last_time: 0.3534  data_time: 0.0486  last_data_time: 0.0704   lr: 4.995e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:05:49 d2.utils.events]: \u001b[0m eta: 0:04:51  iter: 219  total_loss: 1.64  loss_cls: 0.3867  loss_box_reg: 0.6709  loss_mask: 0.487  loss_rpn_cls: 0.04331  loss_rpn_loc: 0.07821    time: 0.3250  last_time: 0.2946  data_time: 0.0403  last_data_time: 0.0092   lr: 5.4945e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:05:56 d2.utils.events]: \u001b[0m eta: 0:04:45  iter: 239  total_loss: 1.717  loss_cls: 0.4097  loss_box_reg: 0.7663  loss_mask: 0.4524  loss_rpn_cls: 0.03438  loss_rpn_loc: 0.06876    time: 0.3259  last_time: 0.3817  data_time: 0.0504  last_data_time: 0.0889   lr: 5.994e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:02 d2.utils.events]: \u001b[0m eta: 0:04:39  iter: 259  total_loss: 1.53  loss_cls: 0.3511  loss_box_reg: 0.6737  loss_mask: 0.416  loss_rpn_cls: 0.0342  loss_rpn_loc: 0.08207    time: 0.3253  last_time: 0.2918  data_time: 0.0410  last_data_time: 0.0107   lr: 6.4935e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:09 d2.utils.events]: \u001b[0m eta: 0:04:33  iter: 279  total_loss: 1.506  loss_cls: 0.3509  loss_box_reg: 0.6585  loss_mask: 0.381  loss_rpn_cls: 0.03644  loss_rpn_loc: 0.07621    time: 0.3249  last_time: 0.2921  data_time: 0.0398  last_data_time: 0.0137   lr: 6.993e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:15 d2.utils.events]: \u001b[0m eta: 0:04:27  iter: 299  total_loss: 1.546  loss_cls: 0.3517  loss_box_reg: 0.7226  loss_mask: 0.3559  loss_rpn_cls: 0.03103  loss_rpn_loc: 0.06915    time: 0.3253  last_time: 0.2959  data_time: 0.0439  last_data_time: 0.0128   lr: 7.4925e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:22 d2.utils.events]: \u001b[0m eta: 0:04:21  iter: 319  total_loss: 1.379  loss_cls: 0.3144  loss_box_reg: 0.621  loss_mask: 0.3227  loss_rpn_cls: 0.03727  loss_rpn_loc: 0.08098    time: 0.3252  last_time: 0.2940  data_time: 0.0438  last_data_time: 0.0115   lr: 7.992e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:28 d2.utils.events]: \u001b[0m eta: 0:04:15  iter: 339  total_loss: 1.431  loss_cls: 0.3292  loss_box_reg: 0.6958  loss_mask: 0.2919  loss_rpn_cls: 0.02718  loss_rpn_loc: 0.07112    time: 0.3252  last_time: 0.3983  data_time: 0.0439  last_data_time: 0.1077   lr: 8.4915e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:35 d2.utils.events]: \u001b[0m eta: 0:04:09  iter: 359  total_loss: 1.253  loss_cls: 0.2905  loss_box_reg: 0.5887  loss_mask: 0.2797  loss_rpn_cls: 0.0275  loss_rpn_loc: 0.06509    time: 0.3253  last_time: 0.2835  data_time: 0.0472  last_data_time: 0.0114   lr: 8.991e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:42 d2.utils.events]: \u001b[0m eta: 0:04:04  iter: 379  total_loss: 1.255  loss_cls: 0.292  loss_box_reg: 0.59  loss_mask: 0.2586  loss_rpn_cls: 0.02491  loss_rpn_loc: 0.07474    time: 0.3261  last_time: 0.4937  data_time: 0.0595  last_data_time: 0.1976   lr: 9.4905e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:48 d2.utils.events]: \u001b[0m eta: 0:03:58  iter: 399  total_loss: 1.154  loss_cls: 0.2542  loss_box_reg: 0.5646  loss_mask: 0.2417  loss_rpn_cls: 0.02814  loss_rpn_loc: 0.07011    time: 0.3264  last_time: 0.4016  data_time: 0.0513  last_data_time: 0.1091   lr: 9.99e-05  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:06:55 d2.utils.events]: \u001b[0m eta: 0:03:52  iter: 419  total_loss: 1.126  loss_cls: 0.2675  loss_box_reg: 0.529  loss_mask: 0.2439  loss_rpn_cls: 0.02457  loss_rpn_loc: 0.06433    time: 0.3265  last_time: 0.2977  data_time: 0.0493  last_data_time: 0.0079   lr: 0.0001049  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:01 d2.utils.events]: \u001b[0m eta: 0:03:46  iter: 439  total_loss: 1.081  loss_cls: 0.261  loss_box_reg: 0.4651  loss_mask: 0.243  loss_rpn_cls: 0.02239  loss_rpn_loc: 0.06171    time: 0.3265  last_time: 0.2742  data_time: 0.0457  last_data_time: 0.0112   lr: 0.00010989  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:08 d2.utils.events]: \u001b[0m eta: 0:03:40  iter: 459  total_loss: 0.9759  loss_cls: 0.2363  loss_box_reg: 0.4303  loss_mask: 0.2255  loss_rpn_cls: 0.01879  loss_rpn_loc: 0.05061    time: 0.3259  last_time: 0.3583  data_time: 0.0340  last_data_time: 0.0721   lr: 0.00011489  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:14 d2.utils.events]: \u001b[0m eta: 0:03:34  iter: 479  total_loss: 0.9837  loss_cls: 0.2353  loss_box_reg: 0.41  loss_mask: 0.2231  loss_rpn_cls: 0.02069  loss_rpn_loc: 0.07145    time: 0.3262  last_time: 0.2982  data_time: 0.0465  last_data_time: 0.0105   lr: 0.00011988  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:21 d2.utils.events]: \u001b[0m eta: 0:03:28  iter: 499  total_loss: 0.9734  loss_cls: 0.2314  loss_box_reg: 0.3975  loss_mask: 0.2275  loss_rpn_cls: 0.01538  loss_rpn_loc: 0.05858    time: 0.3261  last_time: 0.3009  data_time: 0.0439  last_data_time: 0.0108   lr: 0.00012488  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:27 d2.utils.events]: \u001b[0m eta: 0:03:22  iter: 519  total_loss: 0.9116  loss_cls: 0.2115  loss_box_reg: 0.396  loss_mask: 0.2134  loss_rpn_cls: 0.01885  loss_rpn_loc: 0.05503    time: 0.3259  last_time: 0.3138  data_time: 0.0454  last_data_time: 0.0418   lr: 0.00012987  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:34 d2.utils.events]: \u001b[0m eta: 0:03:16  iter: 539  total_loss: 0.9376  loss_cls: 0.2312  loss_box_reg: 0.3856  loss_mask: 0.2125  loss_rpn_cls: 0.01691  loss_rpn_loc: 0.06518    time: 0.3263  last_time: 0.2962  data_time: 0.0513  last_data_time: 0.0134   lr: 0.00013487  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:40 d2.utils.events]: \u001b[0m eta: 0:03:10  iter: 559  total_loss: 0.8591  loss_cls: 0.2117  loss_box_reg: 0.3876  loss_mask: 0.2058  loss_rpn_cls: 0.01413  loss_rpn_loc: 0.05791    time: 0.3260  last_time: 0.2960  data_time: 0.0348  last_data_time: 0.0137   lr: 0.00013986  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:47 d2.utils.events]: \u001b[0m eta: 0:03:04  iter: 579  total_loss: 0.8284  loss_cls: 0.1979  loss_box_reg: 0.3588  loss_mask: 0.2012  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.05459    time: 0.3258  last_time: 0.3557  data_time: 0.0412  last_data_time: 0.0720   lr: 0.00014486  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:07:53 d2.utils.events]: \u001b[0m eta: 0:02:58  iter: 599  total_loss: 0.8447  loss_cls: 0.1959  loss_box_reg: 0.3627  loss_mask: 0.1958  loss_rpn_cls: 0.01312  loss_rpn_loc: 0.05654    time: 0.3258  last_time: 0.2996  data_time: 0.0408  last_data_time: 0.0105   lr: 0.00014985  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:08:00 d2.utils.events]: \u001b[0m eta: 0:02:52  iter: 619  total_loss: 0.8672  loss_cls: 0.209  loss_box_reg: 0.3681  loss_mask: 0.2003  loss_rpn_cls: 0.01694  loss_rpn_loc: 0.06001    time: 0.3260  last_time: 0.2915  data_time: 0.0482  last_data_time: 0.0124   lr: 0.00015485  max_mem: 5305M\n",
      "\u001b[32m[01/11 22:08:07 d2.utils.events]: \u001b[0m eta: 0:02:46  iter: 639  total_loss: 0.8222  loss_cls: 0.1882  loss_box_reg: 0.3439  loss_mask: 0.1963  loss_rpn_cls: 0.01454  loss_rpn_loc: 0.06211    time: 0.3261  last_time: 0.3943  data_time: 0.0466  last_data_time: 0.1040   lr: 0.00015984  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:08:13 d2.utils.events]: \u001b[0m eta: 0:02:40  iter: 659  total_loss: 0.798  loss_cls: 0.1944  loss_box_reg: 0.3285  loss_mask: 0.1919  loss_rpn_cls: 0.01071  loss_rpn_loc: 0.0473    time: 0.3261  last_time: 0.2936  data_time: 0.0403  last_data_time: 0.0128   lr: 0.00016484  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:08:20 d2.utils.events]: \u001b[0m eta: 0:02:35  iter: 679  total_loss: 0.8514  loss_cls: 0.1991  loss_box_reg: 0.3505  loss_mask: 0.2021  loss_rpn_cls: 0.01048  loss_rpn_loc: 0.06123    time: 0.3263  last_time: 0.3555  data_time: 0.0492  last_data_time: 0.0633   lr: 0.00016983  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:08:26 d2.utils.events]: \u001b[0m eta: 0:02:29  iter: 699  total_loss: 0.8349  loss_cls: 0.184  loss_box_reg: 0.312  loss_mask: 0.1878  loss_rpn_cls: 0.0124  loss_rpn_loc: 0.05176    time: 0.3265  last_time: 0.3617  data_time: 0.0524  last_data_time: 0.0692   lr: 0.00017483  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:08:33 d2.utils.events]: \u001b[0m eta: 0:02:23  iter: 719  total_loss: 0.7578  loss_cls: 0.18  loss_box_reg: 0.313  loss_mask: 0.1837  loss_rpn_cls: 0.01257  loss_rpn_loc: 0.05243    time: 0.3263  last_time: 0.4087  data_time: 0.0414  last_data_time: 0.1196   lr: 0.00017982  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:08:39 d2.utils.events]: \u001b[0m eta: 0:02:17  iter: 739  total_loss: 0.741  loss_cls: 0.1862  loss_box_reg: 0.3086  loss_mask: 0.1967  loss_rpn_cls: 0.01392  loss_rpn_loc: 0.06364    time: 0.3263  last_time: 0.2830  data_time: 0.0439  last_data_time: 0.0103   lr: 0.00018482  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:08:46 d2.utils.events]: \u001b[0m eta: 0:02:11  iter: 759  total_loss: 0.7325  loss_cls: 0.176  loss_box_reg: 0.3099  loss_mask: 0.1878  loss_rpn_cls: 0.01235  loss_rpn_loc: 0.04765    time: 0.3260  last_time: 0.2815  data_time: 0.0348  last_data_time: 0.0095   lr: 0.00018981  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:08:52 d2.utils.events]: \u001b[0m eta: 0:02:05  iter: 779  total_loss: 0.7413  loss_cls: 0.1772  loss_box_reg: 0.3202  loss_mask: 0.1843  loss_rpn_cls: 0.01082  loss_rpn_loc: 0.05076    time: 0.3260  last_time: 0.3029  data_time: 0.0429  last_data_time: 0.0119   lr: 0.00019481  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:08:59 d2.utils.events]: \u001b[0m eta: 0:01:59  iter: 799  total_loss: 0.7274  loss_cls: 0.1868  loss_box_reg: 0.3018  loss_mask: 0.1855  loss_rpn_cls: 0.01011  loss_rpn_loc: 0.04824    time: 0.3261  last_time: 0.2997  data_time: 0.0452  last_data_time: 0.0152   lr: 0.0001998  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:05 d2.utils.events]: \u001b[0m eta: 0:01:53  iter: 819  total_loss: 0.7268  loss_cls: 0.175  loss_box_reg: 0.2991  loss_mask: 0.1692  loss_rpn_cls: 0.01452  loss_rpn_loc: 0.0471    time: 0.3261  last_time: 0.3617  data_time: 0.0461  last_data_time: 0.0694   lr: 0.0002048  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:12 d2.utils.events]: \u001b[0m eta: 0:01:47  iter: 839  total_loss: 0.6938  loss_cls: 0.167  loss_box_reg: 0.3003  loss_mask: 0.1799  loss_rpn_cls: 0.01175  loss_rpn_loc: 0.04606    time: 0.3261  last_time: 0.2974  data_time: 0.0410  last_data_time: 0.0144   lr: 0.00020979  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:18 d2.utils.events]: \u001b[0m eta: 0:01:41  iter: 859  total_loss: 0.6833  loss_cls: 0.149  loss_box_reg: 0.2851  loss_mask: 0.1747  loss_rpn_cls: 0.008794  loss_rpn_loc: 0.04424    time: 0.3259  last_time: 0.2978  data_time: 0.0351  last_data_time: 0.0097   lr: 0.00021479  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:25 d2.utils.events]: \u001b[0m eta: 0:01:35  iter: 879  total_loss: 0.6859  loss_cls: 0.1777  loss_box_reg: 0.2941  loss_mask: 0.1744  loss_rpn_cls: 0.009433  loss_rpn_loc: 0.04855    time: 0.3262  last_time: 0.2773  data_time: 0.0598  last_data_time: 0.0109   lr: 0.00021978  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:32 d2.utils.events]: \u001b[0m eta: 0:01:29  iter: 899  total_loss: 0.6529  loss_cls: 0.1538  loss_box_reg: 0.2687  loss_mask: 0.171  loss_rpn_cls: 0.009144  loss_rpn_loc: 0.04256    time: 0.3262  last_time: 0.2832  data_time: 0.0436  last_data_time: 0.0103   lr: 0.00022478  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:38 d2.utils.events]: \u001b[0m eta: 0:01:23  iter: 919  total_loss: 0.6614  loss_cls: 0.1634  loss_box_reg: 0.2687  loss_mask: 0.1675  loss_rpn_cls: 0.009695  loss_rpn_loc: 0.04556    time: 0.3262  last_time: 0.3010  data_time: 0.0434  last_data_time: 0.0126   lr: 0.00022977  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:45 d2.utils.events]: \u001b[0m eta: 0:01:17  iter: 939  total_loss: 0.6538  loss_cls: 0.1679  loss_box_reg: 0.2803  loss_mask: 0.1632  loss_rpn_cls: 0.007192  loss_rpn_loc: 0.04076    time: 0.3264  last_time: 0.3663  data_time: 0.0519  last_data_time: 0.0812   lr: 0.00023477  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:51 d2.utils.events]: \u001b[0m eta: 0:01:11  iter: 959  total_loss: 0.6806  loss_cls: 0.1524  loss_box_reg: 0.2922  loss_mask: 0.1783  loss_rpn_cls: 0.00883  loss_rpn_loc: 0.04364    time: 0.3262  last_time: 0.3020  data_time: 0.0326  last_data_time: 0.0096   lr: 0.00023976  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:09:58 d2.utils.events]: \u001b[0m eta: 0:01:05  iter: 979  total_loss: 0.6394  loss_cls: 0.147  loss_box_reg: 0.2615  loss_mask: 0.1721  loss_rpn_cls: 0.007384  loss_rpn_loc: 0.04876    time: 0.3262  last_time: 0.3044  data_time: 0.0425  last_data_time: 0.0435   lr: 0.00024476  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:10:05 d2.utils.events]: \u001b[0m eta: 0:00:59  iter: 999  total_loss: 0.685  loss_cls: 0.1581  loss_box_reg: 0.2844  loss_mask: 0.1695  loss_rpn_cls: 0.006915  loss_rpn_loc: 0.04492    time: 0.3265  last_time: 0.4088  data_time: 0.0570  last_data_time: 0.1161   lr: 0.00024975  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:10:11 d2.utils.events]: \u001b[0m eta: 0:00:53  iter: 1019  total_loss: 0.6396  loss_cls: 0.1437  loss_box_reg: 0.2642  loss_mask: 0.1659  loss_rpn_cls: 0.009096  loss_rpn_loc: 0.04366    time: 0.3265  last_time: 0.4320  data_time: 0.0420  last_data_time: 0.1468   lr: 0.00025  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:10:18 d2.utils.events]: \u001b[0m eta: 0:00:47  iter: 1039  total_loss: 0.6202  loss_cls: 0.1506  loss_box_reg: 0.26  loss_mask: 0.1588  loss_rpn_cls: 0.006728  loss_rpn_loc: 0.04045    time: 0.3264  last_time: 0.3068  data_time: 0.0371  last_data_time: 0.0120   lr: 0.00025  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:10:24 d2.utils.events]: \u001b[0m eta: 0:00:42  iter: 1059  total_loss: 0.6182  loss_cls: 0.1494  loss_box_reg: 0.2687  loss_mask: 0.155  loss_rpn_cls: 0.007251  loss_rpn_loc: 0.04786    time: 0.3264  last_time: 0.3003  data_time: 0.0399  last_data_time: 0.0102   lr: 0.00025  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:10:31 d2.utils.events]: \u001b[0m eta: 0:00:35  iter: 1079  total_loss: 0.6135  loss_cls: 0.1426  loss_box_reg: 0.2494  loss_mask: 0.1561  loss_rpn_cls: 0.007863  loss_rpn_loc: 0.04321    time: 0.3264  last_time: 0.2966  data_time: 0.0451  last_data_time: 0.0126   lr: 0.00025  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:10:37 d2.utils.events]: \u001b[0m eta: 0:00:29  iter: 1099  total_loss: 0.6209  loss_cls: 0.1484  loss_box_reg: 0.2495  loss_mask: 0.1632  loss_rpn_cls: 0.006407  loss_rpn_loc: 0.03971    time: 0.3263  last_time: 0.3031  data_time: 0.0444  last_data_time: 0.0121   lr: 0.00025  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:10:44 d2.utils.events]: \u001b[0m eta: 0:00:24  iter: 1119  total_loss: 0.5575  loss_cls: 0.1202  loss_box_reg: 0.2313  loss_mask: 0.1513  loss_rpn_cls: 0.005639  loss_rpn_loc: 0.03968    time: 0.3263  last_time: 0.4230  data_time: 0.0380  last_data_time: 0.1275   lr: 0.00025  max_mem: 5318M\n",
      "\u001b[32m[01/11 22:10:50 d2.utils.events]: \u001b[0m eta: 0:00:18  iter: 1139  total_loss: 0.5769  loss_cls: 0.1367  loss_box_reg: 0.2368  loss_mask: 0.1591  loss_rpn_cls: 0.005681  loss_rpn_loc: 0.04243    time: 0.3262  last_time: 0.2984  data_time: 0.0386  last_data_time: 0.0132   lr: 0.00025  max_mem: 5319M\n",
      "\u001b[32m[01/11 22:10:57 d2.utils.events]: \u001b[0m eta: 0:00:12  iter: 1159  total_loss: 0.5989  loss_cls: 0.1388  loss_box_reg: 0.2557  loss_mask: 0.1538  loss_rpn_cls: 0.008012  loss_rpn_loc: 0.03952    time: 0.3262  last_time: 0.3008  data_time: 0.0413  last_data_time: 0.0103   lr: 0.00025  max_mem: 5319M\n",
      "\u001b[32m[01/11 22:11:03 d2.utils.events]: \u001b[0m eta: 0:00:06  iter: 1179  total_loss: 0.5471  loss_cls: 0.1232  loss_box_reg: 0.2258  loss_mask: 0.1537  loss_rpn_cls: 0.004691  loss_rpn_loc: 0.04237    time: 0.3260  last_time: 0.2926  data_time: 0.0366  last_data_time: 0.0108   lr: 0.00025  max_mem: 5319M\n",
      "\u001b[32m[01/11 22:11:10 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 1199  total_loss: 0.6015  loss_cls: 0.1317  loss_box_reg: 0.2411  loss_mask: 0.1546  loss_rpn_cls: 0.008263  loss_rpn_loc: 0.05593    time: 0.3261  last_time: 0.3002  data_time: 0.0519  last_data_time: 0.0119   lr: 0.00025  max_mem: 5319M\n",
      "\u001b[32m[01/11 22:11:10 d2.engine.hooks]: \u001b[0mOverall training speed: 1198 iterations in 0:06:30 (0.3262 s / it)\n",
      "\u001b[32m[01/11 22:11:10 d2.engine.hooks]: \u001b[0mTotal training time: 0:06:32 (0:00:01 on hooks)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/11 22:17:20 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output/model_final.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuai/miniconda3/envs/blasting/lib/python3.12/site-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/11 22:18:56 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[01/11 22:18:56 d2.data.datasets.coco]: \u001b[0mLoaded 7 images in COCO format from /home/shuai/blasting/Dataset/val_dataset_coco/annotations.json\n",
      "\u001b[32m[01/11 22:18:56 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   smoke    | 75           |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[01/11 22:18:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[01/11 22:18:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[01/11 22:18:56 d2.data.common]: \u001b[0mSerializing 7 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/11 22:18:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.01 MiB\n",
      "\u001b[32m[01/11 22:18:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 7 batches\n",
      "\u001b[32m[01/11 22:18:57 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:00.206515 (0.103258 s / iter per device, on 1 devices)\n",
      "\u001b[32m[01/11 22:18:57 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:00 (0.035072 s / iter per device, on 1 devices)\n",
      "\u001b[32m[01/11 22:18:57 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[01/11 22:18:57 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[01/11 22:18:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.179\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.268\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.227\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.162\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.273\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.049\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.257\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.289\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.176\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.393\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "\u001b[32m[01/11 22:18:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 17.922 | 26.757 | 22.722 | 16.231 | 27.258 | 0.000 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.195\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.279\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.220\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.150\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.290\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.051\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.272\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.304\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.412\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "\u001b[32m[01/11 22:18:57 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:------:|:-----:|\n",
      "| 19.458 | 27.913 | 22.039 | 14.952 | 29.014 | 0.000 |\n",
      "OrderedDict({'bbox': {'AP': 17.922423826098324, 'AP50': 26.757098522883055, 'AP75': 22.721560545178207, 'APs': 16.230551626591232, 'APm': 27.258183169571232, 'APl': 0.0}, 'segm': {'AP': 19.45766717178682, 'AP50': 27.913144518188936, 'AP75': 22.039355678728832, 'APs': 14.951757813143951, 'APm': 29.01441582501781, 'APl': 0.0}})\n"
     ]
    }
   ],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"my_dataset_val\", output_dir=\"./output\")\n",
    "val_loader = build_detection_test_loader(cfg, \"my_dataset_val\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
